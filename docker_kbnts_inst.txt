# install docker all nodes
# install containerd

# containerd.io installed allready along with docker

# had to set repo_gpgcheck=0 in kubernetes.repo
# add the kubernetes repo
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=0
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF

# Set SELinux in permissive mode (effectively disabling it)
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

# turn swap off
sudo swapoff -a
sudo vim /etc/fstab
comment /swapfile
sudo sysctl --system

# install kubeadm kubectl etc
sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

# An overlay-filesystem tries to present a filesystem which is the result over overlaying one filesystem on top of the other
# br_netfilter module is required to enable transparent masquerading and to facilitate Virtual Extensible LAN (VxLAN) traffic for communication between Kubernetes pods across the cluster

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

# modprobe intelligently adds or removes a module from the Linux kernel: note that for convenience, there is no differencebetween _ and - in module names

sudo modprobe overlay
sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
sudo swapoff -a
# Apply sysctl params without reboot
sudo sysctl --system

# kubelet enable - will start only after kubeadm init
# before that will get red line error
sudo systemctl restart containerd
sudo systemctl enable --now kubelet

# from calico
# have to do this on all nodes to /etc/containerd/config.toml
#disabled_plugins = ["cri"]

sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --apiserver-advertise-address 192.168.56.2
# [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'

# To start using your cluster, you need to run the following as a regular user:
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# You should now deploy a pod network to the cluster.
# Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
# https://kubernetes.io/docs/concepts/cluster-administration/addons/

# Then you can join any number of worker nodes by running the following on each as root:
sudo kubeadm join 192.168.56.2:6443 --token ecjnjb.cztedgizo8gxe121 \
        --discovery-token-ca-cert-hash sha256:d57f4200b29fd345e31c048362e5721c71d550b9fc485efa646ed4f64009c664

# check the pods - there will be none in default namespace
# coredns in kube-system will run after a cni plugin is enabled
kubectl get pods
kubectl get pods --all-namespaces

# Install the Tigera Calico operator and custom resource definitions.

kubectl create -f https://projectcalico.docs.tigera.io/manifests/tigera-operator.yaml
Install Calico by creating the necessary custom resource. For more information on configuration options available in this manifest, see the installation reference.

kubectl create -f https://projectcalico.docs.tigera.io/manifests/custom-resources.yaml
Note: Before creating this manifest, read its contents and make sure its settings are correct for your environment. For example, you may need to change the default IP pool CIDR to match your pod network CIDR

watch kubectl get pods -n calico-system
# this will have some pods - calico-kube-controllers as pending until 
# a - we remove the taint from the master control plane pod
kubectl taint nodes --all node-role.kubernetes.io/control-plane-
# or b - we join other nodes to the cluster
# and coredns will also continue to be pending

# now execute the cluster joining command on the worker nodes 
# and we should have coredns as well as calico-kube-controllers running

Wait until each pod has the STATUS of Running

# if we need to reinit kubeadm and join nodes
# need to clean /etc/kubernetes and stop kubelet
# to see some sanity in journalctl at least during development
# clean it using sudo rm -rf /run/log/journal/*

#####################################################################
creating the kubernetes dashboard
#####################################################################
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml

# create admin-user 
kubectl apply -f /vagrant/kube/dashboard-user.yaml

apply these two files from kubectl
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard

# copy .kube/config to a location that can be found in windows
cp ~/.kube/config /vagrant
# set KUBECONFIG run kubectl proxy on windows
export KUBECONFIG=/d/vagpg/kafkasecmon/config
kubectl proxy
get the token using this command
# the token will have to be obtained from the master machine
kubectl -n kubernetes-dashboard create token admin-user

and one can log in at localhost:8001 using the token
# go to the link below
http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/workloads?namespace=default

#############################################
eks
#############################################
windows install kubectl from this link
https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html
curl -o kubectl.exe https://s3.us-west-2.amazonaws.com/amazon-eks/1.22.6/2022-03-09/bin/windows/amd64/kubectl.exe
kubectl version --short --client

kubectl version --short --client
Client Version: v1.22.6-eks-7d68063

# to install eskctl install chocolatey and then 
choco install -y eksctl 

# to install helm
choco install -y kubernetes-helm

# create cluster
eks create cluster -n spark-cluster--region us-east-1 --spot --instance-types=m4.large  --nodes 3
# add the stable repository for helm
helm repo add stable https://charts.helm.sh/stable

# helm hub similar to docker hub, confluent hub
 helm search hub nginx
helm repo add bitnami https://charts.bitnami.com/bitnami
# this will put nginx folder in the location from where it is called
helm pull bitnami/nginx --untar=true
helm install helm-nginx bitnami/nginx

# for spark operateor kubernetes-hdfs etc
helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator
helm install my-release spark-operator/spark-operator --namespace spark-operator --create-namespace
helm install my-release spark-operator/spark-operator --namespace spark-operator --set webhook.enable=true
# to uninstall 
helm delete my-release --namespace spark-operator
# copy spark-pi.yam to /vagrant/kube
kubectl apply -f /vagrant/kube/spark-pi.yaml
kubectl get sparkapplications spark-pi -o=yaml
kubectl describe sparkapplication spark-pi
kubectl get pods
# there will be a driver pod with application name - driver
kubectl logs -f spark-pi-driver
kubectl delete sparkapplication spark-pi

# installing helm on centos
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh

Logging
EFK Stack - Elasticsearch, fluentd, kibana

 
aws autoscaling create-or-update-tags --tags ResourceId=eks-16c10042-65f5-d68d-eac5-aab6eb3567f5,ResourceType=auto-scaling-group,Key="k8s.io/cluster-autoscaler/node-template/label/nodegroup-type",Value=on-demand,PropagateAtLaunch=true

aws autoscaling create-or-update-tags --tags ResourceId=eks-58c10042-63d9-404e-777f-8941eed0e1d5,ResourceType=auto-scaling-group,Key="k8s.io/cluster-autoscaler/node-template/label/nodegroup-type",Value=spot,PropagateAtLaunch=true

helm repo add autoscaler https://kubernetes.github.io/autoscaler
helm upgrade -i medium-ca-aws autoscaler/cluster-autoscaler -f ca-values.yaml
kubectl 
########################################################################
kubectl commands
########################################################################
# everywhere can add -n <namespace name>
kubectl get nodes
kubectl get pods
kubectl get deployments 
kubectl scale --replilcas 4 deployment/test
# but it is kubeadm and pods are working on different ips 
# within the pod network - so this will not work

kubectl expose deployment test --type=LoadBalancer --port=80 --target-port=80 --name=test-load-balance
service/test-load-balance exposed

# get all images in kubernetes registry
kubectl get pods --all-namespaces -o jsonpath="{.items[*].spec.containers[*].image}" |\
tr -s '[[:space:]]' '\n' |\
sort |\
uniq -c

kubectl get deployments hello-world
kubectl describe deployments hello-world

kubectl get replicasets
kubectl describe replicasets

kubectl expose deployment hello-world --type=LoadBalancer --name=my-service

kubectl get services my-service

kubectl describe service my-service

kubectl get pods --all-namespaces -o jsonpath="{.items[*].spec.containers[*].image}" |\
tr -s '[[:space:]]' '\n' |\
sort |\
uniq -c

kubectl config view
kubectl config use-context <context from config>
./bin/docker-image-tool.sh -r sparkimg -t my-tag build
./bin/docker-image-tool.sh -r sparkimg -t my-tag push

docker container run -d -p 5000:5000 --restart=always --name registry registry:2
# cretate service account and grant rights
kubectl create serviceaccount spark
kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=default:spark --namespace=default

# to run from a private registry we need to create a secret 
# log in to docker hub with username and password
# that will create a file ~/.docker/config.json
# from that file create a secret
# we have enabled kubernetes to use the private registry
kubectl create secret generic regcred \
    --from-file=.dockerconfigjson=/vagrant/dockerconfig.json \
    --type=kubernetes.io/dockerconfigjson
# if it is a public image it should run without the secret also

export KCN=k8s://https://74CD38CDF7186E80FA15226FBE7AD57F.gr7.us-east-1.eks.amazonaws.com
export KCN=k8s://https://192.168.56.2:6443 
// check out running sparkpi
spark-submit \
    --master k8s://https://74CD38CDF7186E80FA15226FBE7AD57F.gr7.us-east-1.eks.amazonaws.com \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=2 \
    --conf spark.kubernetes.namespace=default \
    --conf spark.kubernetes.executor.request.cores=300m \
    --conf spark.kubernetes.executor.limit.cores=1 \
    --conf spark.kubernetes.driver.pod.name=sparkpi \
    --conf spark.kubernetes.container.image=samar67/spark:spark3r2 \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  \
    --conf spark.kubernetes.submission.waitAppCompletion=false \
    local:///opt/spark/examples/jars/spark-examples_2.13-3.3.0.jar

# the configuration option to add the secret
# --conf spark.kubernetes.container.image.pullSecrets=regcred \
# spark hdfs log processor
spark-submit \
    --master k8s://https://192.168.56.2:6443 \
    --deploy-mode cluster \
    --name sparkhlp \
    --class core.SparkLogProcessor \
    --conf spark.executor.instances=2 \
    --conf spark.kubernetes.executor.request.cores=300m \
    --conf spark.kubernetes.executor.limit.cores=1 \
    --conf spark.kubernetes.container.image="samar67/spark:spark3r4" \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  \
    --conf spark.kubernetes.submission.waitAppCompletion=false \
    --conf spark.kubernetes.driver.pod.name=sparkhlp \
    local:///opt/spark/examples/jars/sparkcore.jar hdfs://192.168.56.2:8020/user/vagrant/apachelogs50k.gz

# process logs and save badrecs
# spark log processor with logging
# in the Dockerfile add vagrant user
RUN useradd -u 1000 -d /opt/spark -G root vagrant

spark-submit \
    --master k8s://https://192.168.56.2:6443 \
    --deploy-mode cluster \
    --name sparklpwl \
    --class core.SparkLogProcessorWithLogging \
    --conf spark.executor.instances=2 \
    --conf spark.kubernetes.executor.request.cores=300m \
    --conf spark.kubernetes.executor.limit.cores=1 \
    --conf spark.kubernetes.container.image="samar67/spark:spark3r4" \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  \
    --conf spark.kubernetes.submission.waitAppCompletion=false \
    --conf spark.kubernetes.driver.pod.name=sparklpwl \
    local:///opt/spark/examples/jars/sparkcore.jar hdfs://192.168.56.2:8020/user/vagrant/apachelogs50k.gz hdfs://192.168.56.2:8020/user/vagrant/badrecs

kubectl create secret generic aws-secret --from-literal=key=--from-literal=secret=

# submitting to fetch data and store results in s3
spark-submit \
    --master k8s://https://192.168.56.2:6443 \
    --deploy-mode cluster \
    --name spark-s3check \
    --class SparkS3AOpsWConf \
    --conf spark.executor.instances=2 \
    --conf spark.kubernetes.executor.request.cores=500m \
    --conf spark.kubernetes.executor.limit.cores=1 \
    --conf spark.kubernetes.container.image="samar67/spark:spark3r4" \
    --conf spark.kubernetes.driver.pod.name=sparks3ch \
    --conf spark.kubernetes.namespace=default \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  \
    --conf spark.kubernetes.driver.secretKeyRef.
    ACCESS_KEY_ID=aws-secret:key \
    --conf spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-secret:secret \
    --conf spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID=aws-secret:key \
    --conf spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-secret:secret \
    --conf spark.kubernetes.submission.waitAppCompletion=false \
    local:///opt/spark/examples/jars/sparks3.jar  s3a://expb/shak.txt s3a://expb/shak2475 INFO

# submitting to fetch data and store results in s3 and testing the packages option
# continue to get class not found error
spark-submit \
    --master k8s://https://192.168.56.2:6443 \
    --deploy-mode cluster \
    --name spark-s3check \
    --class SparkS3AOpsWConf \
    --conf spark.executor.instances=2 \
    --conf spark.kubernetes.executor.request.cores=500m \
    --conf spark.kubernetes.executor.limit.cores=1 \
    --conf spark.kubernetes.container.image="samar67/spark:spark3r2" \
    --conf spark.kubernetes.driver.pod.name=sparks3ch \
    --conf spark.kubernetes.namespace=default \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  \
    --conf spark.kubernetes.driver.secretKeyRef.AWS_ACCESS_KEY_ID=aws-secret:key \
    --conf spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-secret:secret \
    --conf spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID=aws-secret:key \
    --conf spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-secret:secret \
    --conf spark.kubernetes.file.upload.path=s3a://expb/awsdeps \
    --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
    --conf spark.hadoop.fs.s3a.fast.upload=true \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    --conf spark.kubernetes.submission.waitAppCompletion=false \
    --packages org.apache.hadoop:hadoop-aws:3.3.2 \
    local:///opt/spark/examples/jars/sparks3.jar  s3a://expb/shak.txt s3a://expb/shak2475 INFO

###############################################################
kuberenetes checking insecure registry
###############################################################
ran registry container on admin
added daemon.json to /etc/docker
pushed hello world and nginx image to admin:5000/hwis and admin:5000/mnginx
then in test deployment changes image to admin:5000/mnginx
only led to errimagepull
registry of type admin:5000/mnginx 
does not work

now will try and run a tls enabled self signed registry
# in vagrant make a directory certs
mkdir -p certs

# generate key and certificate
# for this to work had to install openssl-1.1.1 on admin machine
openssl req \
  -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key \
  -addext "subjectAltName = DNS:master.e4rlearning.com" \
  -x509 -days 365 -out certs/domain.crt

docker container rm -f registry
docker image rm admin.e4rlearning.com/mnginx

# copied from docker - working - below adding htpasswd
docker run -d \
  --restart=always \
  --name registry \
  -v "$(pwd)"/certs:/certs \
  -e REGISTRY_HTTP_ADDR=0.0.0.0:443 \
  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \
  -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \
  -p 443:443 \
  registry:2

docker run -d \
  --restart=always \
  --name registry \
  -v "$(pwd)"/certs:/certs \
  -v "$(pwd)"/auth:/auth \
  -e REGISTRY_AUTH=htpasswd \
  -e REGISTRY_AUTH_HTPASSWD_REALM="master.e4rlearning.com" \
  -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \
  -e REGISTRY_HTTP_ADDR=0.0.0.0:5000 \
  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \
  -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \
  -p 5000:5000 \
  registry:2
docker login -u docker -p docker https://master.e4rlearning.com:5000
Error response from daemon: Get "https://admin.e4rlearning.com:443/v2/": x509: certificate signed by unknown authority

  docker tag nginx admin.e4rlearning.com/mnginx
  docker push admin.e4rlearning.com/mnginx
# above works but kubernetes deplouyment using this image does not work
openssl req -new -x509 -nodes -sha1 -days 365 -key domain.key -out domain.crt -subj "/C=IN/ST=Delhi/L=ORN/O=TGI/CN=admin.e4rlearning.com"
cd .. && mkdir auth
docker run -d \
  --restart=always \
  --name registry \
  -v `pwd`/auth:/auth \
  -v `pwd`/certs:/certs \
  -v `pwd`/certs:/certs \
  -e REGISTRY_AUTH=htpasswd \
  -e REGISTRY_AUTH_HTPASSWD_REALM="admin.e4rlearning.com" \
  -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \
  -e REGISTRY_HTTP_ADDR=0.0.0.0:5000 \
  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \
  -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \
  -p 5000:5000 \
  registry:2

#############################################
install openssl 1.1.1 on centos admin machine
#############################################
https://gist.github.com/fernandoaleman/5459173e24d59b45ae2cfc618e20fe06
sudo yum install -y make gcc perl-core pcre-devel wget zlib-devel
wget https://ftp.openssl.org/source/openssl-1.1.1k.tar.gz
-- had to add --no-check-certificate to the line above
tar -xzvf openssl-1.1.1k.tar.gz
Configure, build and install OpenSSL
Uncompress the source file

tar -xzvf openssl-1.1.1k.tar.gz
Change to the OpenSSL directory

cd openssl-1.1.1k
Configure the package for compilation

./config --prefix=/usr --openssldir=/etc/ssl --libdir=lib no-shared zlib-dynamic
Compile package

make
Test compiled package

make test
Install compiled package

sudo make install
Export library path
Create environment variable file

sudo vim /etc/profile.d/openssl.sh
Add the following content

export LD_LIBRARY_PATH=/usr/local/lib:/usr/local/lib64
Load the environment variable

source /etc/profile.d/openssl.sh
Verify the OpenSSL version
openssl version
#############################################


#############################################
working with private registry ond dockerhub.io
#############################################
docker login
and copied ~/.docker/config.json to /vagrant/dockerconfig.json




kubectl get secret regcred --output=yaml

then there is privatereg.yaml which uses this secret
and the deployment works

docker login
docker tag nginx samar67/mnginx
docker push samar67/mnginx
#############################################


spark-submit --master k8s://https://74CD38CDF7186E80FA15226FBE7AD57F.gr7.us-east-1.eks.amazonaws.com --deploy-mode cluster --name spark-pi --class org.apache.spark.examples.SparkPi --conf spark.executor.instances=2 --conf spark.kubernetes.namespace=default --conf spark.kubernetes.executor.request.cores=800m --conf spark.kubernetes.executor.limit.cores=1 --conf spark.kubernetes.driver.pod.name=sparkpi --conf spark.kubernetes.container.image=samar67/spark:spark3r2 --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  --conf spark.kubernetes.submission.waitAppCompletion=false local:///opt/spark/examples/jars/spark-examples_2.13-3.3.0.jar

kubectl create secret generic aws-secret --from-literal=key= --from-literal=secret=

spark-submit --master k8s://https://74CD38CDF7186E80FA15226FBE7AD57F.gr7.us-east-1.eks.amazonaws.com --deploy-mode cluster --name spark-s3check --class SparkS3AOpsWConf --conf spark.executor.instances=2 --conf spark.kubernetes.executor.request.cores=500m --conf spark.kubernetes.executor.limit.cores=1 --conf spark.kubernetes.container.image="samar67/spark:spark3r4" --conf spark.kubernetes.driver.pod.name=sparks3ch --conf spark.kubernetes.namespace=default --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  --conf spark.kubernetes.driver.secretKeyRef.AWS_ACCESS_KEY_ID=aws-secret:key --conf spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-secret:secret --conf spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID=aws-secret:key --conf spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY=aws-secret:secret --conf spark.kubernetes.submission.waitAppCompletion=false local:///opt/spark/examples/jars/sparks3.jar  s3a://fbucketn/shakespeare.txt s3a://fbucketn/shakwc1 INFO

spark-submit --master k8s://https://74CD38CDF7186E80FA15226FBE7AD57F.gr7.us-east-1.eks.amazonaws.com --deploy-mode cluster --name sparklpwl --class core.SparkLogProcessorWithLogging --conf spark.executor.instances=2 --conf spark.kubernetes.executor.request.cores=300m --conf spark.kubernetes.executor.limit.cores=1 --conf spark.kubernetes.container.image="samar67/spark:spark3r4" --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  --conf spark.kubernetes.submission.waitAppCompletion=false --conf spark.kubernetes.driver.pod.name=sparklpwl local:///opt/spark/examples/jars/sparkcore.jar hdfs://192.168.56.2:8020/user/vagrant/apachelogs50k.gz hdfs://192.168.56.2:8020/user/vagrant/badrecsaws
