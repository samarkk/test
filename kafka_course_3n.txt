export bdir="/tmp/confluent.934030"

########################################################################
start zookeeper and kafka server
########################################################################

# start zookeeper  as a backgroud daemon process
zookeeper-server-start -daemon /home/samar/confluent/etc/kafka/zookeeper.properties

# verify port 2181 occupied
# tlpg is only a synonym for sudo netstat -tulpn | grep 
tlpg 2181

# the logs are in the logs directory under confluent
ls -l $bdir/zookeeper/logs/
# we can check the logs to undrestand problems and generally see what's going on

# zookeeper-shell localhost:2181
# ls / 
# we shall see brokers, topics as they are created

# start kafka server
# check out the server.properties file in the kafka configuration directory
less /home/samar/confluent/etc/kafka/server.properties
# broker.id has to be different for each broker
# same zookeeper.connect will make different brokers part of a cluster

# start kafka-server  as a daemon
kafka-server-start -daemon /home/samar/confluent/etc/kafka/server.properties
# verify port 9092 occupied
tlpg 9092

# logs will be in $bdir/kafka/logs in server.log
tail -f /home/samar/confluent/logs/server.log

# check out the broker node in the zookeeper shell

-----------------------------------------------------------------------

########################################################################
create, delete, topics
########################################################################
kafka-topics --bootstrap-server localhost:9092 --list

kafka-topics --bootstrap-server localhost:9092 --create --topic first-topic --partitions 3 --replication-factor 1

kafka-topics --bootstrap-server localhost:9092 --topic first-topic --describe

kafka-topics --bootstrap-server localhost:9092 --topic --delete first-topic

########################################################################
tmux elementary commands and navigation
########################################################################
Ctrl+b " - split window into two panes horizontally
Ctrl+b % - split window into two panes vertically
Ctrl+b Ctrl+ArrowKey - resize in direction of arrow
Ctrl+b ArrowKey - move to the pane in direction of arrow
Ctrl+b o - move cursor to other pane
Ctrl+b q + pane-number - move to the numbered pane

########################################################################
 check the kafka console consumer and kafka console producer in action
########################################################################
tmux and createt two panes
in one pane
kafka-console-producer --bootstrap-server localhost:9092 --topic first-topic
in other pane
kafka-console-consumer --bootstrap-server localhost:9092 --topic first-topic
in the producer pane, type in stuff and see it appearing alongside in the consumer pane


########################################################################
explore kafka consumer groups
########################################################################
# set up tmux to have producer in one pane and three consumers, 
part of a consumer group, one each in a different pane
# start producer to write to a topic
kafka-console-producer --topic first-topic --bootstrap-server localhost:9092
# one by one start a consumer in a group
kafka-console-consumer --bootstrap-server localhost:9092 --topic first-topic --group firstgroup --from-beginning
# set up three more panes and start three more consumers to consume the
topic 
kafka-console-consumer --bootstrap-server localhost:9092 --topic first-topic --group secondgroup --from-beginning
# divide the producer pane into two and start another producer to write to the same topic
# shut down all and now explore producer and consumer in consumer groups
with messages produced with a key
kafka-console-producer --bootstrap-server localhost:9092 --topic key-topic --property parse.key=true --property key.separator=, --property ignore.errors=true
# start three consumers 
kafka-console-consumer --bootstrap-server localhost:9092 --topic key-topic --property print.key=true --from-beginning

########################################################################
 Java producer and consumer API
########################################################################
* check out the producer demo - ProducerDemo.java
* producer demo with keys - ProducerDemoKeysCallback.java

* check out ConsumerDemo - ConsumerDemo.java
* Consumer seek to an offset for a particular partition - ConsumerAssignAndSeek.java
* Cleanup using wakeupException with Consumer running on a separate thread - ConsumerDemoWithThread.java
* Admin client and checking, printing, resetting offsets - ConsumerDemoWithAdminClient.java
* Reebalance listener called on partition assignment and revocation - ConsumerDemoWRBL.java

* Send lines from a file to a Kafka topic - ProducerFileClient.java


################################################################
set up Cassandra and use it as the sink with the consumer API
################################################################

# check and start cassandra 
# just issue 
cassandra

# creating a keyspace, a demo table and inserting some values
# launch cqlsh
CREATE KEYSPACE testks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
create table ttbl(sunsign text, id int, fname text, lname text, primary key (sunsign, id));
insert into ttbl(sunsign, id, fname, lname) values ('libra', 1, 'amitabh', 'bacchan');
insert into ttbl(sunsign, id, fname, lname) values ('libra', 2, 'hema', 'malini');

# create finks - finance keyspace
CREATE KEYSPACE finks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
# create fotable in finks
CREATE TABLE finks.fotable (
    symbol text,
    expiry date,
    trdate date,
    instrument text,
    option_typ text,
    strike_pr decimal,
    chgoi int,
    contracts int,
    cpr decimal,
    oi int,
    trdval decimal,
    tstamp timestamp,
   PRIMARY KEY ((symbol, expiry), trdate, instrument, option_typ, strike_pr)
);
# to connect to remote host
set broadcast_rpc_address to public ip;
for vm set it to the network adapter ip alotted
set rpc_address to private ip - 10.0.0.4/5 etc;
for vm set it to the ip gotten from network adapter
set listen_address to localhost
after this cqlsh will fail connecting to 127.0.0.1
so use
cqlsh myVM 9042
where myVM is the hostname and 9042 is the port
the private ip will also work
the public ip also does
so in vm use 
cqlsh 192.168.181.138 9042

run ConsumerCassandra.java to send messages from kafka nsefotopic to cassandra fotbl

################################################################
MySQL sink with the consumer API
################################################################

# mysql sink 
# follow the steps in fotbl_mysql_create.sql to create set up for sending nsefotopic to mysql
# run ConsumerMySQL.java to send the topic messages to mysql

# set up for sending data to hdfs sink
# go to hadoop conf directory
cd /home/samar/hadoop/etc/hadoop
# replace the ip used with the new ip assigned by  wsl
sed -i s/<existing ip>/<vm ip>/g core-site.xml
sed -i s/<existing ip>/<vm ip>/g hdfs-site.xml
# start hdfs daemons
hdfs --daemon start namenode
hdfs --daemon start datanode
# run ConsumerHDFSSink.java to send nsefotopic messages to hdfs
# one may get hadoop_home not set in windows, winutils may not be installed
# worked after giving hadoop home not set error
# one solution is to package the jar, add hadoop classpath and run the class
# or on the windows machine download winutils and set hadoop home
############################################################
start schema-registry and docker schema registry container
################################################################
# start schema-registry
schema-registry-start -daemon /home/samar/confluent/etc/schema-registry/schema-registry.properties

* Create a Generic Record to a topic - GenericRecordProducer.java
* check out schema-registry rest api

# see the schemas
curl 192.168.181.138:8081/subjects
# see versions for a schema
curl 192.168.181.138:8081/subjects/<subject-name eg gcustomer-value>/versions
# see the schema for a particular version
curl 192.168.181.138:8081/subjects/<subject-name eg gcustomer-value>/versions/1
# Delete a schema version - first soft delete 
curl -X DELETE 192.168.181.138:8081/subjects/<subject-name eg gcustomer-value>/versions/1
# Permanently delete a schema version
curl -X DELETE 192.168.181.138:8081/subjects/<subject-name eg gcustomer-value>/versions/1?permanent=true

* use avro specific reocrds to populate topic with schemas
* check pom.xml where we have avro-maven-plugin 
* it will generate the java avro schemas for schemas in src/main/resources/avro folder
* check out the customer.avsc 
* use AvroProducer.Java to create a Kafka topic and record for the customer schema
* in customer.avsc and AvroProducer.Java comment the version 1 code and check that we are able to work with version 2 to see avro schema evolution in action
* user AvroProducerFileClient.java to load fo01JAN2918bhav.csv to nsefotopic_avro, use nseforec.avsc for the nse fo avro specific record

###################################################################
start multiple brokres for the cluster
###################################################################
go to /home/samar/confluent/etc/kafka/
copy server.properties to server1.properties
in server1.properties change broker.id to 1 
and replace 9092 with 9093
uncomment the two lines below and change 8090 to 8091
#confluent.metadata.server.listeners=http://0.0.0.0:8090
#confluent.metadata.server.advertised.listeners=http://127.0.0.1:8090

start kafka server 2 
replicate with server2.properties, port 9094 and 8092 and start kafka server 3

start docker kafka server and check the three servers active

##################################################################
check topic configurations
##################################################################
# see the options
kafka-configs

# create a test_config_topic to check out configurations
kafka-topics --bootstrap-server localhost:2181 --create --topic test_config_topic --partitions 3 --replication-factor 1

# describe configs for topic
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name test_config_topic --describe

# add a configuration
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name test_config_topic  --add-config min.insync.replicas=2 --alter

# check the configuration
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name test_config_topic --describe

# see configurations using kafka-topics
kafka-topics --bootstrap-server localhost:2181 --topic test_config_topic --describe

# delete configuration
kafka-configs --bootstrap-server localhost:9092 --entity-type topics --entity-name test_config_topic  --delete-config min.insync.replicas --alter

# verify config deleted
kafka-topics --bootstrap-server localhost:2181 --topic test_config_topic --bootstrap-server
# delete the topic
kafka-topics --bootstrap-server localhost:2181 --delete --topic test_config_topic

######################################################################
log compaction check out
######################################################################
kafka-topics --bootstrap-server localhost:2181 --create --topic shippinig_address --partitions 1 --replication-factor 1 --config cleanup.policy=compact --config min.cleanable.dirty.ratio=0.001 --config segment.ms=5000

kafka-console-producer --broker-list localhost:9092 --topic shippinig_address --property parse.key=true --property key.separator=,

kafka-console-consumer --bootstrap-server localhost:9092 --topic shippinig_address --property print.key=true --property key.separator=,

######################################################################
Kafka streams
######################################################################
run WordCountApp.java to check basic kafka streams topology
run kafka console producer on the input topic and kafka console consumer on the output topic simultaneously

run WordCountTWApp.java to check window operations
run kafka console producer and consumer on input and output topics simultaneously

######################################################################
Streams Streams Join
######################################################################
create topics - impressions, clicks, impressionsandclicks

kafka-topics --bootstrap-server localhost:9092 --create --topic impressions --partitions 3 --replication-factor 1
kafka-topics --bootstrap-server localhost:9092 --create --topic clicks --partitions 3 --replication-factor 1
kafka-topics --bootstrap-server localhost:9092 --create --topic impressionsandclicks --partitions 3 --replication-factor 1

kafka-console-producer --bootstrap-server localhost:9092 --topic impressions --property parse.key=true --property key.separator=, --property ignore.error=true 

kafka-console-producer --bootstrap-server localhost:9092 --topic clicks --property parse.key=true --property key.separator=, --property ignore.error=true 

kafka-console-consumer --bootstrap-server localhost:9092 --topic impressionsandclicks --property print.key=true --from-beginning 

kafka-streams-application-reset --bootstrap-servers localhost:9092 --application-id ssjapp

kafka-topics --bootstrap-server localhost:9092 --delete --topic impressions

kafka-topics --bootstrap-server localhost:9092 --delete --topic clicks 

kafka-topics --bootstrap-server localhost:9092 --delete --topic impressionsandclicks 

run kafka console consumer for each of the three topics

and run StreamStreamJoinDemo.java

######################################################################
Streams Table Join
######################################################################
create topics user-regions, user-clicks and user-clicks-regions

kafka-topics --bootstrap-server localhost:9092 --create --topic user-regions --partitions 3 --replication-factor 1
kafka-topics --bootstrap-server localhost:9092 --create --topic user-clicks --partitions 3 --replication-factor 1
kafka-topics --bootstrap-server localhost:9092 --create --topic user-clicks-regions --partitions 3 --replication-factor 1

run kafka console consumer for each of the three topics

run StreamTableJoinDemo.java and then StreamTableJoinDriver.Demo

##################################################################
Table Table Join 
##################################################################
create topics - player-team, player-score, player-team-score

kafka-topics --bootstrap-server localhost:9092 --create --topic player-team -partitions 3 --replication-factor 1
kafka-topics --bootstrap-server localhost:9092 --create --topic player-score --partitions 3 --replication-factor 1
kafka-topics --bootstrap-server localhost:9092 --create --topic player-team-score --partitions 3 --replication-factor 1

Run TableTableJoin.java

##################################################################
Stream Globak KTable Join 
##################################################################
create topics userp and usert   --purchases and table

kafka-topics --bootstrap-server localhost:9092 --create --topic userp -partitions 3 --replication-factor 1

kafka-topics --bootstrap-server localhost:9092 --create --topic usert -partitions 3 --replication-factor 1

create topic upej - user purchase enrichment join
kafka-topics --bootstrap-server localhost:9092 --create --topic upej -partitions 3 --replication-factor 1

run UEE.java // UserEventEnrichment.java
run UserDataProducer.java

consume topic upej and check

##################################################################
Streams branching
#################################################################
create topics nsefotopic_avro and gr-1mn and lakh-to-mn and upto1lakh

kafka-topics --bootstrap-server localhost:9092 --create --topic nsefotopic_avro -partitions 3 --replication-factor 1

kafka-topics --bootstrap-server localhost:9092 --create --topic gr-1mn -partitions 3 --replication-factor 1

kafka-topics --bootstrap-server localhost:9092 --create --topic lakh-to-mn -partitions 3 --replication-factor 1

kafka-topics --bootstrap-server localhost:9092 --create --topic upto1lakh -partitions 3 --replication-factor 1

run AvroProducerFileClient.java to populate nsefotopic_avro
and kafka console consumer for the it plus the three branch topics


##################################################################
Streaming exactly once app
##################################################################
create topics bank-transactions and bank-balance-exactly-once

kafka-topics --bootstrap-server master:2181 --create --topic bank-trasactions -partitions 3 --replication-factor 1
kafka-topics --bootstrap-server master:2181 --create --topic bank-balance-exactly-once -partitions 3 --replication-factor 1

run BankBalanceExactlyOnceApp.java
run BankTransactionsProducer.java

check the bank-balance-exactly-once topic
kafka-console-consumer --bootstrap-server localhost:9092 --topic bank-balance-exactly-once --property print.key=true

##################################################################
Strreams Stateless App 
##################################################################
# creae topics tweets, crypto-sentiment
# in statelessprocessing package
# run TwitterSentiment.java
# run TweetsProducer.java 

kafka-topics --bootstrap-server master:9092 --delete --topic players
kafka-topics --bootstrap-server master:9092 --delete --topic products

kafka-topics --bootstrap-server master:9092 --create --topic tweets --partitions 2 --replication-factor 1 

kafka-topics --bootstrap-server master:9092 --create --topic crypto-sentiment --partitions 2 --replication-factor 1 

kafka-console-consumer --bootstrap-server master:9092 --topic tweets --from-beginning --property print.key=true
kafka-console-consumer --bootstrap-server master:9092 --topic crypto-sentiment --from-beginning --property print.key=true

##############################################################
Streams Stateful App
##############################################################
# create topics players,products,score-events,high-scores
# use players_generator, products_generator, scores_generator to populate the player, products, score-events topic
# Run LeaderboardApp.java and LeaderboardService
# Check out the outputs and the endpoints 

kafka-topics --bootstrap-server master:9092 --delete --topic players
kafka-topics --bootstrap-server master:9092 --delete --topic products
kafka-topics --bootstrap-server master:9092 --delete --topic score-events
kafka-topics --bootstrap-server master:9092 --delete --topic high-scores

kafka-topics --bootstrap-server master:9092 --create --topic score-events --partitions 2 --replication-factor 1 

kafka-topics --bootstrap-server master:9092 --create --topic high-scores --partitions 2 --replication-factor 1 

kafka-topics --bootstrap-server master:9092 --create --topic players --partitions 2 --replication-factor 1 

kafka-topics --bootstrap-server master:9092 --create --topic products --partitions 2 --replication-factor 1 --config cleanup.policy=compact

node /vagrant/players_generator.js | kafka-console-producer --bootstrap-server master:9092 --topic players --property parse.key=true --property key.separator=,

node /vagrant/products_generator.js | kafka-console-producer --bootstrap-server master:9092 --topic products --property parse.key=true --property key.separator=,

node /vagrant/scores_generator.js 20 1 100 | kafka-console-producer --bootstrap-server master:9092 --topic score-events

kafka-console-consumer --bootstrap-server master:9092 --topic players --from-beginning --property print.key=true
kafka-console-consumer --bootstrap-server master:9092 --topic products --from-beginning --property print.key=true
kafka-console-consumer --bootstrap-server master:9092 --topic score-events --from-beginning --property print.key=true
kafka-console-consumer --bootstrap-server master:9092 --topic high-scores --from-beginning --property print.key=true
kafka-console-consumer --bootstrap-server master:9092 --topic scores-with-players --from-beginning --property print.key=true

#############################################################
Time and Windows Patient Monitoring App
#############################################################
# create topics pulse-events body-temp-events and alerts
# produce data using body_temp_generator, pulse_events_generator or use the json files
# Run App.java in timeprocessing and RestService for interactive queries

kafka-topics --bootstrap-server master:9092 --create --topic pulse-events --partitions 2 --replication-factor 1 

kafka-topics --bootstrap-server master:9092 --create --topic body-temp-events --partitions 2 --replication-factor 1 

kafka-topics --bootstrap-server master:9092 --create --topic alerts --partitions 2 --replication-factor 1 

cat /vagrant/pulse-events.json | kafka-console-producer --bootstrap-server master:9092 --topic pulse-events --property parse.key=true --property key.separator='|'
kafka-console-consumer --bootstrap-server master:9092 --topic pulse-events --from-beginning --property print.key=true

cat /vagrant/body-temp-events.json | kafka-console-producer --bootstrap-server master:9092 --topic body-temp-events --property parse.key=true --property key.separator='|'
kafka-console-consumer --bootstrap-server master:9092 --topic body-temp-events --from-beginning --property print.key=true

node pulse_events_generator.js 10 100 100  | kafka-console-producer --bootstrap-server master:9092 --topic pulse-events  --property parse.key=true --property key.separator=,

node body_temp_generator.js | kafka-console-producer --bootstrap-server master:9092 --topic body-temp-events --property parse.key=true --property key.separator=,

##################################################################
Important resources - URLs
##################################################################

https://dev.to/confluentinc/5-things-every-apache-kafka-developer-should-know-4nb

https://www.confluent.io/blog/incremental-cooperative-rebalancing-in-kafka/

https://www.confluent.io/blog/cooperative-rebalancing-in-kafka-streams-consumer-ksqldb/

https://medium.com/bakdata/solving-my-weird-kafka-rebalancing-problems-c05e99535435
