mysql --local-infile=1 -u root -pramanShastri24! < /vagrant/spark-python/scripts/mysql_mock_table_creation.sql
https://github.com/samarkk/spark-python/blob/main/sparksql/fo_cm_analysis.py
# we want to copy 201819 directory to hdfs
# to copy large nested structures it is better to use hadoop distcp
# it runs a map reduce operation to copy the directory in parallel to the target destination in hdfs
# we will need to start yarn first
# be in the master machine and issue
/home/vagrant/hadoop/sbin/start-yarn.sh
# then do hadoop distcp
hadoop distcp file:///vagrant/201819 hdfs://master:8020/user/vagrant

# please ensure to run this on master
hive --service metastore 2>&1 &>/dev/null &
tlpg 9083
# this below is to check the log - to terminate this press ctrl and c together
tail -f /tmp/vagrant/hive.log


# run spark history server on the admin machine
/home/vagrant/spark/sbin/start-history-server.sh
hdfs dfs -mkdir pkgdemo
hdfs dfs -put /vagrant/spark-python/logparse.py pkgdemo
spark-submit --master yarn --num-executors 3 --executor-memory 1400m --conf spark.memory.fraction=0.9 /vagrant/spark-python/SparkLogProcessorModPkg.py hdfs://master:8020/user/vagrant/pkgdemo apachelogs.gz

shak_words = shak_rdd.flatMap(lambda x: x.split('  '))
shak_words.take(5)
shak_tuples = shak_words.map(lambda x: (x, 1))
shak_tuples.take(5)
shak_words_count = shak_tuples.reduceByKey(lambda x, y:  x + y)
shak_words_count.take(5)
shak_words_count_sorted = shak_words_count.sortBy(lambda x: -x[1])
shak_words_count_sorted.take(20)
sc.textFile('shakespeare.txt').flatMap(lambda x: x.split(' ')).filter(lambda x: x != '').map(lambda x: (x,1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: -x[1]).take(20)

# this is where will be sharing code, commands for the MB workshop
192.168.50.2  master.e4rlearning.com master
192.168.50.3  node1.e4rlearning.com node1
192.168.50.4  node2.e4rlearning.com node2
192.168.50.5  admin.e4rlearning.com admin
192.168.50.6  node3.e4rlearning.com node3
open anaconda prompt
pyspark
ardd = sc.parallelize(range(1000))
ardd.sum()
open git command
cd \vagpg\centoshbase
vagrant up admin master node1 node2
https://github.com/samarkk/test/blob/master/hadoop_comms.txt
shak_rdd = sc.textFile('shakespeare.txt')
shak_rdd.take(5)
export PYSPARK_PYTHON=/usr/bin/python3
export PYSPARK_DRIVER_PYTHON=/usr/bin/python3
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
https://github.com/samarkk/spark-python/blob/main/RDDTransformations.py
https://github.com/samarkk/spark-python/blob/main/RDDActons.py
https://github.com/samarkk/spark-python/blob/main/SparkLogProcessor.py
https://github.com/samarkk/spark-python/blob/main/SparkLogProcessorMod.py
https://github.com/samarkk/spark-python/blob/main/SparkLogProcessorModPkg.py
https://github.com/samarkk/spark-python/blob/main/logparse.py



