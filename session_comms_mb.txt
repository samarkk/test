shak_words = shak_rdd.flatMap(lambda x: x.split('  '))
shak_words.take(5)
shak_tuples = shak_words.map(lambda x: (x, 1))
shak_tuples.take(5)
shak_words_count = shak_tuples.reduceByKey(lambda x, y:  x + y)
shak_words_count.take(5)
shak_words_count_sorted = shak_words_count.sortBy(lambda x: -x[1])
shak_words_count_sorted.take(20)

# this is where will be sharing code, commands for the MB workshop
192.168.50.2  master.e4rlearning.com master
192.168.50.3  node1.e4rlearning.com node1
192.168.50.4  node2.e4rlearning.com node2
192.168.50.5  admin.e4rlearning.com admin
192.168.50.6  node3.e4rlearning.com node3
open anaconda prompt
pyspark
ardd = sc.parallelize(range(1000))
ardd.sum()
open git command
cd \vagpg\centoshbase
vagrant up admin master node1 node2
https://github.com/samarkk/test/blob/master/hadoop_comms.txt
shak_rdd = sc.textFile('shakespeare.txt')
shak_rdd.take(5)
export PYSPARK_PYTHON=/usr/bin/python3
export PYSPARK_DRIVER_PYTHON=/usr/bin/python3
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
https://github.com/samarkk/spark-python/blob/main/RDDTransformations.py
https://github.com/samarkk/spark-python/blob/main/RDDActons.py
https://github.com/samarkk/spark-python/blob/main/SparkLogProcessor.py
https://github.com/samarkk/spark-python/blob/main/SparkLogProcessorMod.py
https://github.com/samarkk/spark-python/blob/main/SparkLogProcessorModPkg.py
https://github.com/samarkk/spark-python/blob/main/logparse.py



