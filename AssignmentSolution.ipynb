{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['SPARK_HOME']='/home/samar/spark'\n",
    "os.environ['PYLIB']=os.environ['SPARK_HOME']+'/python/lib'\n",
    "sys.path.insert(0,os.environ['PYLIB']+'/py4j-0.10.9-src.zip')\n",
    "sys.path.insert(1,os.environ['PYLIB']+'/pyspark.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"AssignmentAnswers\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version 3.0.1 and sc version 3.0.1\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "print(f'spark version {spark.version} and sc version {sc.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Find all words in shakespare.txt that have a length greater than ten. Find out the count for each of these words by creating a rdd which will have elements such as ('somelongword',23) etc. where 23 is the number of somelongword present in the data. Find out the sum of the product of word length and word count for these pairs.\n",
    "Provide the code and the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileloc = 'file:///home/samar/shakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173338"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('file:///home/samar/shakespeare.txt').flatMap(lambda x: x.split(' ')).filter(lambda x: len(x) > 10)\\\n",
    ".map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\\\n",
    ".map(lambda x: (len(x[0]) * x[1])).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Save the first ten thousand lines of apachelogs in a file apachelogs10k. Copy this file in HDFS. Find out the average request size per host for the days present in this file. You will have to compute the number of hosts by day and the total request size by day and divide the total request size by number of hosts to come to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] \"GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0\" 200 1839'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apachelogs10k = sc.textFile('apachelogs10k')\n",
    "apachelogs10k.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "month_map = {'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n",
    "    'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12}\n",
    "\n",
    "def parse_apache_time(s):\n",
    "    \"\"\" Convert Apache time format into a Python datetime object\n",
    "    Args:\n",
    "        s (str): date and time in Apache time format\n",
    "    Returns:\n",
    "        datetime: datetime object (ignore timezone for now)\n",
    "    \"\"\"\n",
    "    return datetime.datetime(int(s[7:11]),\n",
    "                             month_map[s[3:6]],\n",
    "                             int(s[0:2]),\n",
    "                             int(s[12:14]),\n",
    "                             int(s[15:17]),\n",
    "                             int(s[18:20]))\n",
    "\n",
    "\n",
    "def parseApacheLogLine(logline):\n",
    "    \"\"\" Parse a line in the Apache Common Log format\n",
    "    Args:\n",
    "        logline (str): a line of text in the Apache Common Log format\n",
    "    Returns:\n",
    "        tuple: either a dictionary containing the parts of the Apache Access Log and 1,\n",
    "               or the original invalid log line and 0\n",
    "    \"\"\"\n",
    "    match = re.search(APACHE_ACCESS_LOG_PATTERN, logline)\n",
    "    if match is None:\n",
    "        return (logline, 0)\n",
    "    size_field = match.group(9)\n",
    "    if size_field == '-':\n",
    "        size = int(0)\n",
    "    else:\n",
    "        size = int(match.group(9))\n",
    "    return (Row(\n",
    "        host          = match.group(1),\n",
    "        client_identd = match.group(2),\n",
    "        user_id       = match.group(3),\n",
    "        date_time     = parse_apache_time(match.group(4)),\n",
    "        method        = match.group(5),\n",
    "        endpoint      = match.group(6),\n",
    "        protocol      = match.group(7),\n",
    "        response_code = int(match.group(8)),\n",
    "        content_size  = size\n",
    "    ), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A regular expression pattern to extract fields from the log line\n",
    "APACHE_ACCESS_LOG_PATTERN = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)\\s*(\\S*)\" (\\d{3}) (\\S+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 10000 lines, successfully parsed 10000 lines, failed to parse 0 lines\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "logFile = \"hdfs://localhost:8020/user/samar/apachelogs10k\"\n",
    "# logFileRDD = sc.textFile(fileLoc)\n",
    "# logFile = os.path.join(baseDir, inputPath)\n",
    "\n",
    "def parseLogs():\n",
    "    \"\"\" Read and parse log file \"\"\"\n",
    "    parsed_logs = (sc\n",
    "                   .textFile(logFile)\n",
    "                   .map(parseApacheLogLine)\n",
    "                   .cache())\n",
    "\n",
    "    access_logs = (parsed_logs\n",
    "                   .filter(lambda s: s[1] == 1)\n",
    "                   .map(lambda s: s[0])\n",
    "                   .cache())\n",
    "\n",
    "    failed_logs = (parsed_logs\n",
    "                   .filter(lambda s: s[1] == 0)\n",
    "                   .map(lambda s: s[0]))\n",
    "    failed_logs_count = failed_logs.count()\n",
    "    if failed_logs_count > 0:\n",
    "        print('Number of invalid logline: %d' % failed_logs.count())\n",
    "        for line in failed_logs.take(20):\n",
    "            print('Invalid logline: %s' % line)\n",
    "\n",
    "    print('Read %d lines, successfully parsed %d lines, failed to parse %d lines' % \n",
    "          (parsed_logs.count(), access_logs.count(), failed_logs.count()))\n",
    "    return parsed_logs, access_logs, failed_logs\n",
    "\n",
    "\n",
    "parsed_logs, access_logs, failed_logs = parseLogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 853)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dailyHosts = access_logs.map(lambda x: (x.date_time.day,x.host)).distinct().\\\n",
    "map(lambda x: (x[0], 1)).reduceByKey(lambda x, y: x + y)\n",
    "dailyHosts.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 179517299)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dailySize = access_logs.map(lambda x: (x.date_time.day, x.content_size)).reduceByKey(lambda x, y: x + y)\n",
    "dailySize.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "AvgDailyRequestSizePerHost = dailyHosts.join(dailySize).map(lambda x: (x[0], x[1][1]/x[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 210454.04337631888)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AvgDailyRequestSizePerHost.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-6 - for these exercises use the following list as the data \n",
    "['INFY,2018-01-01,100,100071', 'INFY,2018-01-02,120,107130', 'INFY,2018-01-03,110,124175', 'INFY,2018-01-02,115,109175', 'BHEL,2018-01-01,4005,21000', 'BHEL,2018-01-02,5240,22015', 'BHEL,2018-01-03,5287,23450', 'BHEL,2018-01-04,6480,28156']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Create a RDD from the above list and map it to tuple of 4 by splitting each element on comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('INFY', '2018-01-01', 100, 100071.0), ('INFY', '2018-01-02', 120, 107130.0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StocksList = ['INFY,2018-01-01,100,100071', 'INFY,2018-01-02,120,107130', \n",
    "              'INFY,2018-01-03,110,124175', 'INFY,2018-01-02,115,109175', \n",
    "              'BHEL,2018-01-01,4005,21000', 'BHEL,2018-01-02,5240,22015', \n",
    "              'BHEL,2018-01-03,5287,23450', 'BHEL,2018-01-04,6480,28156']\n",
    "StocksRDD = sc.parallelize(StocksList).map(lambda x: x.split(',')).map(\n",
    "lambda x: (x[0],x[1],int(x[2]),float(x[3])))\n",
    "StocksRDD.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Define a schema to represent the dataframe that you will create from the rdd. Give the schema fields - stock of type string, date of type string, qty of type int, vlu of type double. Use the rdd and schema to create a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_schema = StructType([StructField('stock', StringType()),\n",
    "                           StructField('date', StringType()),\n",
    "                           StructField('qty', IntegerType()),\n",
    "                           StructField('vlu', DoubleType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_df = spark.createDataFrame(StocksRDD, stocks_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Create a temporary table|view for this dataframe. And find the percentile for quantity traded (qty) by stock for this table, dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_df.createOrReplaceTempView('stocktbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----+--------------------+--------+\n",
      "|stock|      date| qty|stock_qty_percentile|     vlu|\n",
      "+-----+----------+----+--------------------+--------+\n",
      "| INFY|2018-01-01| 100|                 0.0|100071.0|\n",
      "| INFY|2018-01-03| 110|  0.3333333333333333|124175.0|\n",
      "| INFY|2018-01-02| 115|  0.6666666666666666|109175.0|\n",
      "| INFY|2018-01-02| 120|                 1.0|107130.0|\n",
      "| BHEL|2018-01-01|4005|                 0.0| 21000.0|\n",
      "| BHEL|2018-01-02|5240|  0.3333333333333333| 22015.0|\n",
      "| BHEL|2018-01-03|5287|  0.6666666666666666| 23450.0|\n",
      "| BHEL|2018-01-04|6480|                 1.0| 28156.0|\n",
      "+-----+----------+----+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select stock,date,qty, percent_rank() over(partition by stock order by qty) as stock_qty_percentile,\n",
    "    vlu\n",
    "    from stocktbl \n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Use arrow pandas_udf to find sum and average of vlu for the above table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def sum_udf(v: pd.Series) -> float:\n",
    "    return v.sum()\n",
    "\n",
    "@pandas_udf('double')\n",
    "def avg_udf(v: pd.Series) ->  float:\n",
    "    return v.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|totalvlu| avgvlu|\n",
      "+--------+-------+\n",
      "|535172.0|66896.5|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks_df.select(sum_udf('vlu').alias('totalvlu'), avg_udf('vlu').alias('avgvlu')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---------+\n",
      "|stock|totalvlu|   avgvlu|\n",
      "+-----+--------+---------+\n",
      "| INFY|440551.0|110137.75|\n",
      "| BHEL| 94621.0| 23655.25|\n",
      "+-----+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks_df.groupBy('stock').\\\n",
    "agg(sum_udf('vlu').alias('totalvlu'), avg_udf('vlu').alias('avgvlu')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
